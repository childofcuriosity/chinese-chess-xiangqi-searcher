import subprocess
import re
import sys
import os
import copy
import time
import random
import urllib.request
# --- 1. 配置与显示颜色 ---
CLOUD_BOOK_ENABLED=1 # 是否启用云开局库查询
USE_DEPTH=0  # 是否使用固定深度搜索 (否则使用迭代加深) （测棋力对打要开）
LONG_MAX_DEPTH=6  # 非固定深度时的最大搜索深度
OPEN_NMP=1  # 是否启用空步裁剪 (Null Move Pruning)
LONG_MAX_TIME=75.0 # 非固定深度时的3步后默认最大思考时间 (秒)，可以根据需要调整


ROWS = 10
COLS = 9



PIECE_CHARS = {
    'R': '车', 'N': '马', 'B': '相', 'A': '仕', 'K': '帅', 'C': '炮', 'P': '兵',
    'r': '车', 'n': '马', 'b': '象', 'a': '士', 'k': '将', 'c': '炮', 'p': '卒',
    '.': '．' 
}
SCORE_INF = 30000 


# --- 2. 核心参数 (基于成熟引擎标准) ---

# --- 1. 修正后的基础子力价值 ---
# 逻辑：
# 1. 兵(未过河) = 60 ~ 80。过河后价值由 PST 决定(可达 150+)。
# 2. 车(1000) = 绝对主力。
# 3. 马/炮(450) = 约为车的 45%。
# 4. 士象(120) = 略高于未过河兵，防止 AI 为了贪吃弃士象（导致被杀）。

# --- 1. 基于 ElephantEye (象眼) 比例调整的子力价值 ---
# 解释：
# 1. 兵(100) 是基准。
# 2. 士/象(250) 必须很高！防止 AI 用士象去换对方的兵。丢士象=输一半。
# 3. 车(1000) 依然是王。
# 4. 马/炮(450) 保持标准。
PIECE_VALUES = {
    'k': 10000,     
    'r': 4, 
    'n': 3, 
    'c': 3, 
    'a': 2,  
    'b': 2,  
    'p': 1, 
    'K': 10000, 'R': 4, 'N': 3, 'C': 3, 'A': 2, 'B': 2, 'P': 1
}
#看象眼，子力价值都是靠位置价值表（PST）来体现的。我就简单只给王加分为了保王
# --- PST (Piece-Square Tables) 位置价值表 ---
# 所有的表都是基于红方视角 (Row 0是底线, Row 9是敌方底线)
# 黑方使用时，代码会自动翻转 (Row = 9 - Row)
# --- 修正后的 PST (绝对坐标版) ---
# 此时：Row 9 是红方底线，Row 0 是红方要进攻的敌方底线
# 逻辑：红方棋子在 Row r 时，直接查 table[r]
# --- 基于“梦入神蛋”库生成的 PST (平均值版) ---
# 坐标系：Row 0 = 敌方底线, Row 9 = 我方底线
# Generated by Residual Learning
# 这些值是【相对于】基础子力价值的加成/减分

# 基础子力设定:
# R: 702
# N: 413
# B: -36
# A: 90
# K: 0
# C: 694
# P: 178

# [兵/卒] Pawn
# 融合了开中局(攻/守)与残局(攻/守)的平均值
# 显著特征：下二路(Row 1)咽喉线价值极高(73分)，过河后(Row 3,4)价值递增
pst_pawn = [
    [ 9,  9,  9, 11, 13, 11,  9,  9,  9,], # Row 0: 敌底 (老卒)
    [ 39, 49, 69, 84, 89, 84, 69, 49, 39, ], # Row 1: 敌下二路 (核心攻击位)
    [ 39, 49, 64, 74, 74, 74, 64, 49, 39, ], # Row 2: 敌宫顶
    [39, 46, 54, 59, 61, 59, 54, 46, 39,], # Row 3: 敌兵林
    [29, 37, 41, 54, 59, 54, 41, 37, 29,], # Row 4: 敌河口 (过河)
    [ 7,  0, 13,  0, 16,  0, 13,  0,  7, ], # Row 5: 我河口
    [  7,  0,  7,  0, 15,  0,  7,  0,  7, ], # Row 6: 我兵林
    [  0,  0,  0,  0,  0,  0,  0,  0,  0], # Row 7
    [  0,  0,  0,  0,  0,  0,  0,  0,  0], # Row 8
    [  0,  0,  0,  0,  0,  0,  0,  0,  0]  # Row 9
]

for i in range(len(pst_pawn)):
    for j in range(len(pst_pawn[i])):
        pst_pawn[i][j] += 10 #补丁

# === 最终结果 ===
# AI (New): 5 (250.0%)
# AI (Old): 2 (100.0%)
# Draws   : 3 (150.0%)
# 结论: 新版 AI 更强！

# [帅/将] King
# 融合了中局与残局
# Row 9 (底线) 最安全，Row 7 (二楼) 在残局有一定助攻分，但总体较低
pst_king = [
    [  0,  0,  0,  0,  0,  0,  0,  0,  0],
    [  0,  0,  0,  0,  0,  0,  0,  0,  0],
    [  0,  0,  0,  0,  0,  0,  0,  0,  0],
    [  0,  0,  0,  0,  0,  0,  0,  0,  0],
    [  0,  0,  0,  0,  0,  0,  0,  0,  0],
    [  0,  0,  0,  0,  0,  0,  0,  0,  0],
    [  0,  0,  0,  0,  0,  0,  0,  0,  0],
    [  0,  0,  0,   1,  1,  1,  0,  0,  0], # Row 7: 宫顶
    [  0,  0,  0,   2,  2,  2,  0,  0,  0], # Row 8: 宫心
    [  0,  0,  0,  11, 15, 11,  0,  0,  0]  # Row 9: 底线
]

# [仕/士] Advisor
# 采用 PromotionThreatless 表 (平均了受威胁与无威胁的情况)
# 仅保留九宫格内的有效位置
pst_advisor = [
    [  0,  0,  0,  0,  0,  0,  0,  0,  0],
    [  0,  0,  0,  0,  0,  0,  0,  0,  0],
    [  0,  0,  0,  0,  0,  0,  0,  0,  0],
    [  0,  0,  0,  0,  0,  0,  0,  0,  0],
    [  0,  0,  0,  0,  0,  0,  0,  0,  0],
    [  0,  0,  0,  0,  0,  0,  0,  0,  0],
    [  0,  0,  0,  0,  0,  0,  0,  0,  0],
    [  0,  0,  0, 30,  0, 30,  0,  0,  0], # Row 7: 羊角士
    [  0,  0,  0,  0, 33,  0,  0,  0,  0], # Row 8: 中士
    [  0,  0,  0, 30,  0, 30,  0,  0,  0]  # Row 9: 底士
]

# [相/象] Bishop
# 采用 PromotionThreatless 表
# 仅保留相位的有效位置 (含河口象，虽危险但原表有分值30)
pst_bishop = [
    [  0,  0,  0,  0,  0,  0,  0,  0,  0],
    [  0,  0,  0,  0,  0,  0,  0,  0,  0],
    [  0,  0,  0,  0,  0,  0,  0,  0,  0],
    [  0,  0,  0,  0,  0,  0,  0,  0,  0],
    [  0,  0,  0,  0,  0,  0,  0,  0,  0],
    [  0,  0, 30,  0,  0,  0, 30,  0,  0], # Row 5: 河口象
    [  0,  0,  0,  0,  0,  0,  0,  0,  0],
    [ 28,  0,  0,  0, 33,  0,  0,  0, 28], # Row 7: 高相/中相
    [  0,  0,  0,  0,  0,  0,  0,  0,  0],
    [  0,  0, 30,  0,  0,  0, 30,  0,  0]  # Row 9: 底相
]

# [马] Knight
# 开中局与残局的平均值
# 卧槽马(Row 1, 2)和河口马(Row 3, 4)分值很高
pst_knight = [
    [  90, 90, 90, 96, 90, 96, 90, 90, 90,], # Row 0: 敌底
    [  90, 96,103, 97, 94, 97,103, 96, 90,], # Row 1: 卧槽/挂角
    [92, 98, 99,103, 99,103, 99, 98, 92, ], # Row 2: 敌宫顶
    [  93,108,100,107,100,107,100,108, 93,], # Row 3: 敌兵林
    [ 93, 99, 99,101,102,101, 99, 99, 93], # Row 4: 敌河口
    [  90,100, 99,103,104,103, 99,100, 90, ], # Row 5: 我河口
    [ 90, 98,101,102,103,102,101, 98, 90, ], # Row 6: 巡河
    [92, 94, 98, 95, 98, 95, 98, 94, 92,], # Row 7: 兵林
    [  85, 90, 92, 93, 78, 93, 92, 90, 85, ], # Row 8
    [  88, 85, 90, 88, 90, 88, 90, 85, 88,]  # Row 9: 归心/底线
]

# [车] Rook
# 开中局与残局的平均值
# 霸王车位置(Row 4)得分最高，底车(Row 0)次之
pst_rook = [
    [206,208,207,213,214,213,207,208,206,], # Row 0: 敌底 (沉底)
    [206,212,209,216,233,216,209,212,206, ], # Row 1: 敌下二路
    [206,208,207,214,216,214,207,208,206, ], # Row 2
    [206,213,213,216,216,216,213,213,206,], # Row 3: 捉子/压制
    [208,211,211,214,215,214,211,211,208,], # Row 4: 敌河口 (霸王车)
    [208,212,212,214,215,214,212,212,208,], # Row 5: 我河口
    [204,209,204,212,214,212,204,209,204,], # Row 6
    [198,208,204,212,212,212,204,208,198,], # Row 7
    [200,208,206,212,200,212,206,208,200,], # Row 8
    [194,206,204,212,200,212,204,206,194,]  # Row 9: 我底
]

# [炮] Cannon
# 开中局与残局的平均值
# 炮在底线(Row 0)和中路/肋道(Col 3,4,5)比较稳健
pst_cannon = [
    [100,100, 96, 91, 90, 91, 96,100,100, ], # Row 0: 敌底
    [98, 98, 96, 92, 89, 92, 96, 98, 98,], # Row 1
    [ 97, 97, 96, 91, 92, 91, 96, 97, 97, ], # Row 2: 炮架位
    [ 96, 99, 99, 98,100, 98, 99, 99, 96,], # Row 3
    [ 96, 96, 96, 96,100, 96, 96, 96, 96, ], # Row 4: 敌河口
    [ 95, 96, 99, 96,100, 96, 99, 96, 95, ], # Row 5: 我河口
    [ 96, 96, 96, 96, 96, 96, 96, 96, 96, ], # Row 6
    [97, 96,100, 99,101, 99,100, 96, 97,], # Row 7: 巡河/兵林
    [96, 97, 98, 98, 98, 98, 98, 97, 96,], # Row 8
    [ 96, 96, 97, 99, 99, 99, 97, 96, 96,]  # Row 9: 我底 (担子炮)
]

PST_MAP = {
    'k': pst_king,   'K': pst_king,
    'r': pst_rook,   'R': pst_rook,
    'n': pst_knight, 'N': pst_knight,
    'c': pst_cannon, 'C': pst_cannon,
    'p': pst_pawn,   'P': pst_pawn,
    'a': pst_advisor,'A': pst_advisor,
    'b': pst_bishop, 'B': pst_bishop
}


# --- 评估权重配置 (基于 Eleeye 简化) ---
USE_RELATION=0  # 是否启用关系与阵型评估
#可能不用才是好的，因为慢，且有hack二阶更加不准，并且pst有一阶的棋形功能了
# 棋形分
EV_HOLLOW_CANNON = 200    # 空头炮 (非常危险)
EV_CENTRAL_CANNON = 50    # 中炮 (镇中)
EV_LINKED_PAWNS = 30      # 连兵 (过河兵相连)
EV_ROOK_TRAPPED = -50     # 车被困 (低机动性)
EV_FULL_GUARDS = 40       # 士象全 (防守加分)

# 威胁分
EV_ATTACK_KING = 20       # 每一个攻击老将区域的大子

# 机动性 (每多一个控制点加的分)
EV_MOBILITY = {
    'r': 6,  'n': 12, 'c': 6, # 车马炮的灵活性价值
    'R': 6,  'N': 12, 'C': 6
}
# --- 3. 主逻辑类 ---
# --- TT 表标记 ---
TT_EXACT = 0   # 精确值
TT_ALPHA = 1   # 上界 (最多这么多分，也就是 Fail Low)
TT_BETA  = 2   # 下界 (至少这么多分，也就是 Fail High)

class XiangqiCLI:

    def __init__(self):

        self.board = [
            ['r', 'n', 'b', 'a', 'k', 'a', 'b', 'n', 'r'],
            ['.', '.', '.', '.', '.', '.', '.', '.', '.'],
            ['.', 'c', '.', '.', '.', '.', '.', 'c', '.'],
            ['p', '.', 'p', '.', 'p', '.', 'p', '.', 'p'],
            ['.', '.', '.', '.', '.', '.', '.', '.', '.'],
            ['.', '.', '.', '.', '.', '.', '.', '.', '.'],
            ['P', '.', 'P', '.', 'P', '.', 'P', '.', 'P'],
            ['.', 'C', '.', '.', '.', '.', '.', 'C', '.'],
            ['.', '.', '.', '.', '.', '.', '.', '.', '.'],
            ['R', 'N', 'B', 'A', 'K', 'A', 'B', 'N', 'R']
        ]
        self.turn = 'red'
        self.player_side = None
        self.game_over = False
        self.current_score = 0
        

        # --- 在类的 __init__ 中修改 ---
        self.tt_size = 1000003  # 一个足够大的素数
        # 每个条目存储: [zobrist_hash, depth, flag, score, best_move]
        # 初始化为 None 或固定长度列表以节省分配开销
        self.tt = [None] * self.tt_size
        # --- Zobrist 与 置换表 初始化 ---
        self.zobrist_table = {} # 存储每个棋子在每个位置的随机数
        self.zobrist_turn = random.getrandbits(64) # 轮到黑方走棋的随机数
        self.current_hash = 0
        # --- 新增：历史局面 Hash 表 ---
        self.history = [] 
        self.init_zobrist() # 生成随机数表
        self.init_score_and_hash() # 计算初始分数和初始Hash

        self.start_time = 0
        self.time_limit = float('inf') # 默认无限制，实际使用时会设置为具体秒数
        self.stop_search = False  # 中断标志
        self.nodes = 0           # 统计搜索量

        self.history_table = [[[[0]*9 for _ in range(10)] for _ in range(9)] for _ in range(10)]
        self.killer_moves = [[None, None] for _ in range(64)]


        

        # 2. 添加一个评估缓存 (非常重要！否则太慢)
        self.eval_cache = {} 
    # def evaluate(self):
    #     """
    #     新的综合评估入口
    #     """
    #     # 1. 基础分 (增量维护的子力+PST)
    #     base = self.current_score
        
    #     # 2. 关系与阵型分 (实时计算)
    #     # 注意：这里如果太慢，可以考虑只在 depth > X 时调用，或者简化
    #     relation = self.get_relation_score()
        
    #     total = base + relation
        
    #     # 如果当前轮到黑方走，minimax 视角需要取反吗？
    #     # 注意：你的 minimax 实现中，maximize_player 是布尔值。
    #     # 如果你的 current_score 已经是 "红优则正，黑优则负"，
    #     # 那么这里直接返回 total 即可。minimax 内部会根据 maximizing_player 处理。
    #     # 这里假设 total 是相对于红方的净胜分。
        
    #     return total
    # 3. 彻底替换 evaluate 函数
    def evaluate(self):
        base = self.current_score
        total = base
        
        
        return total
        
    # 2. 辅助函数：获取移动的历史得分
    def get_history_score(self, move):
        start, end = move
        return self.history_table[start[0]][start[1]][end[0]][end[1]]

    def is_time_up(self):
        """检查是否超时"""
        # 每隔 1024 个节点检查一次时间，减少系统调用开销
        if self.nodes & 1023 == 0:
            if time.time() - self.start_time > self.time_limit:
                self.stop_search = True
        return self.stop_search
    
    def init_zobrist(self):
        """为每个格子上的每种棋子生成一个唯一的 64位 随机整数"""
        pieces = PIECE_CHARS.keys()
        for r in range(ROWS):
            for c in range(COLS):
                for p in pieces:
                    if p != '.':
                        self.zobrist_table[(r, c, p)] = random.getrandbits(64)


    def get_piece_value(self, piece, r, c):
        """辅助函数：获取单个棋子在特定位置的分数（包含子力+PST）"""
        if piece == '.': return 0
        
        val = PIECE_VALUES.get(piece, 0)
        pst_val = 0
        if piece in PST_MAP:
            table = PST_MAP[piece]
            if self.is_red(piece):
                pst_val = table[r][c]
            else:
                pst_val = table[9-r][c] # 黑方翻转
        
        total = val + pst_val
        return total if self.is_red(piece) else -total

    def init_score_and_hash(self):
        """初始化计算 分数 和 Hash"""
        self.current_score = 0
        self.current_hash = 0
        for r in range(ROWS):
            for c in range(COLS):
                p = self.board[r][c]
                if p != '.':
                    self.current_score += self.get_piece_value(p, r, c)
                    self.current_hash ^= self.zobrist_table[(r, c, p)]
        
        # 如果初始是黑方走，需要异或 turn 的随机数（通常开局是红方，不做处理）
        if self.turn == 'black':
            self.current_hash ^= self.zobrist_turn
        self.history = [self.current_hash]    

    def make_move(self, start, end):
        r1, c1 = start
        r2, c2 = end
        moving_piece = self.board[r1][c1]
        captured_piece = self.board[r2][c2]

        # 1. 更新分数 (增量)
        self.current_score -= self.get_piece_value(moving_piece, r1, c1)
        if captured_piece != '.':
            self.current_score -= self.get_piece_value(captured_piece, r2, c2)
        self.current_score += self.get_piece_value(moving_piece, r2, c2)

        # 2. 更新 Hash (核心优化: XOR 是可逆的)
        # 移出起点棋子
        self.current_hash ^= self.zobrist_table[(r1, c1, moving_piece)]
        # 如果终点有子，移出被吃棋子
        if captured_piece != '.':
            self.current_hash ^= self.zobrist_table[(r2, c2, captured_piece)]
        # 移入终点棋子
        self.current_hash ^= self.zobrist_table[(r2, c2, moving_piece)]
        # 切换行动方 Hash
        self.current_hash ^= self.zobrist_turn

        # 3. 执行移动
        self.board[r2][c2] = moving_piece
        self.board[r1][c1] = '.'
        self.turn = 'black' if self.turn == 'red' else 'red'
        
        self.history.append(self.current_hash)

        return captured_piece

    def undo_move(self, start, end, captured):
        self.history.pop()
        r1, c1 = start
        r2, c2 = end
        moved_piece = self.board[r2][c2]

        # 1. 还原分数
        self.current_score -= self.get_piece_value(moved_piece, r2, c2)
        self.current_score += self.get_piece_value(moved_piece, r1, c1)
        if captured != '.':
            self.current_score += self.get_piece_value(captured, r2, c2)

        # 2. 还原 Hash (操作完全对称)
        self.current_hash ^= self.zobrist_turn # 换回原来的行动方
        self.current_hash ^= self.zobrist_table[(r2, c2, moved_piece)] # 移出终点
        if captured != '.':
            self.current_hash ^= self.zobrist_table[(r2, c2, captured)] # 加回被吃子
        self.current_hash ^= self.zobrist_table[(r1, c1, moved_piece)] # 加回起点

        # 3. 还原棋盘
        self.board[r1][c1] = moved_piece
        self.board[r2][c2] = captured
        self.turn = 'black' if self.turn == 'red' else 'red'

    def is_red(self, piece):
        return piece.isupper()

    def in_board(self, r, c):
        return 0 <= r < ROWS and 0 <= c < COLS

    # --- 走法生成 (标准逻辑) ---
    def get_valid_moves(self, r, c):
        piece = self.board[r][c]
        moves = []
        if piece == '.': return moves
        is_red_piece = self.is_red(piece)
        
        def is_teammate(nr, nc):
            p = self.board[nr][nc]
            return p != '.' and self.is_red(p) == is_red_piece

        # 车
        if piece.lower() == 'r':
            for dr, dc in [(0,1),(0,-1),(1,0),(-1,0)]:
                nr, nc = r+dr, c+dc
                while self.in_board(nr, nc):
                    if self.board[nr][nc] == '.': moves.append((nr, nc))
                    else:
                        if not is_teammate(nr, nc): moves.append((nr, nc))
                        break
                    nr, nc = nr+dr, nc+dc
        # 马 (带撇脚)
        elif piece.lower() == 'n':
            for dr, dc, lr, lc in [(-2,-1,-1,0), (-2,1,-1,0), (2,-1,1,0), (2,1,1,0),
                                   (-1,-2,0,-1), (1,-2,0,-1), (-1,2,0,1), (1,2,0,1)]:
                nr, nc, lr, lc = r+dr, c+dc, r+lr, c+lc
                if self.in_board(nr, nc) and self.board[lr][lc] == '.' and not is_teammate(nr, nc):
                    moves.append((nr, nc))
        # 炮
        elif piece.lower() == 'c':
            for dr, dc in [(0,1),(0,-1),(1,0),(-1,0)]:
                nr, nc = r+dr, c+dc
                platform = False
                while self.in_board(nr, nc):
                    if self.board[nr][nc] == '.':
                        if not platform: moves.append((nr, nc))
                    else:
                        if not platform: platform = True
                        else:
                            if not is_teammate(nr, nc): moves.append((nr, nc))
                            break
                    nr, nc = nr+dr, nc+dc
        # 相/象
        elif piece.lower() == 'b':
            for dr, dc, er, ec in [(-2,-2,-1,-1), (-2,2,-1,1), (2,-2,1,-1), (2,2,1,1)]:
                nr, nc, er, ec = r+dr, c+dc, r+er, c+ec
                if self.in_board(nr, nc) and self.board[er][ec] == '.' and not is_teammate(nr, nc):
                    if (is_red_piece and nr>=5) or (not is_red_piece and nr<=4):
                        moves.append((nr, nc))
        # 士/仕
        elif piece.lower() == 'a':
            for dr, dc in [(-1,-1),(-1,1),(1,-1),(1,1)]:
                nr, nc = r+dr, c+dc
                if self.in_board(nr, nc) and 3<=nc<=5 and not is_teammate(nr, nc):
                    if (is_red_piece and 7<=nr<=9) or (not is_red_piece and 0<=nr<=2):
                        moves.append((nr, nc))
        # 帅/将
        elif piece.lower() == 'k':
            for dr, dc in [(0,1),(0,-1),(1,0),(-1,0)]:
                nr, nc = r+dr, c+dc
                if self.in_board(nr, nc) and 3<=nc<=5 and not is_teammate(nr, nc):
                    if (is_red_piece and 7<=nr<=9) or (not is_red_piece and 0<=nr<=2):
                        moves.append((nr, nc))
            # 2. 飞将逻辑 (King Facing King)
            # 红方向上找(-1)，黑方向下找(+1)
            direction = -1 if is_red_piece else 1
            check_r = r + direction
            
            while 0 <= check_r < ROWS:
                target_piece = self.board[check_r][c]
                if target_piece == '.':
                    # 如果是空地，继续往前看
                    check_r += direction
                else:
                    # 碰到棋子了
                    # 如果碰到的是敌方的将/帅，说明可以飞将！
                    enemy_king = 'k' if is_red_piece else 'K'
                    if target_piece == enemy_king:
                        moves.append((check_r, c))
                    # 无论碰到什么子（不管是敌是友，还是敌方老将），
                    # 只要中间有阻隔或已经找到了老将，搜索就结束
                    break
        # 兵/卒
        elif piece.lower() == 'p':
            dr = -1 if is_red_piece else 1
            if self.in_board(r+dr, c) and not is_teammate(r+dr, c): moves.append((r+dr, c))
            if (is_red_piece and r<=4) or (not is_red_piece and r>=5): # 过河后允许平移
                for dc in [-1, 1]:
                    if self.in_board(r, c+dc) and not is_teammate(r, c+dc): moves.append((r, c+dc))
        return moves

    def get_all_moves(self, is_red_turn):
        moves = []
        for r in range(ROWS):
            for c in range(COLS):
                p = self.board[r][c]
                if p != '.' and self.is_red(p) == is_red_turn:
                    ms = self.get_valid_moves(r, c)
                    for m in ms: moves.append(((r,c), m))
        return moves


# --- 5. 静态搜索 (Quiescence Search) ---
    def quiescence_search(self, alpha, beta, maximizing_player, qs_depth=0):
        # 1. 检查是否被将军 (这是 QS 中最昂贵但也最必要的操作，部分引擎会从外部传入状态以优化)
        in_check = self.is_in_check(maximizing_player)

        # 2. Stand Pat (静止评估)
        # 只有在【不被将军】的情况下，才有资格选择“不走棋”
        if not in_check:
            score = self.evaluate() # 建议统一用 evaluate，保证分数标准统一
            
            if maximizing_player:
                if score >= beta: return beta
                if score > alpha: alpha = score
            else:
                if score <= alpha: return alpha
                if score < beta: beta = score
        
        # 3. 深度限制防止爆炸
        # 如果搜太深，强制返回。注意：如果被将军且无路可走，这里返回评估分可能不准，
        # 但为了防止死循环只能妥协。通常 10 层 QS 已经极深了。
        if qs_depth > 10:
            return self.evaluate()

        # 4. 生成着法
        # 优化：通常引擎会有 generate_capture_moves() 和 generate_all_moves() 两个方法
        if in_check:
            # 被将军：必须生成所有逃生步（包括不吃子的移动）
            # 这里的 get_all_moves 必须包含挡将、躲将
            moves = self.get_all_moves(maximizing_player)
        else:
            # 未被将军：只生成吃子步
            # 这里建议优化你的底层代码，不要生成所有步再过滤，直接只生成吃子步效率高很多
            all_moves = self.get_all_moves(maximizing_player)
            moves = []
            for start, end in all_moves:
                # 目标格有子 = 吃子
                if self.board[end[0]][end[1]] != '.':
                    moves.append((start, end))

        # MVV-LVA 排序
        moves.sort(key=lambda m: PIECE_VALUES.get(self.board[m[1][0]][m[1][1]], 0), reverse=True)

        # 5. 遍历着法
        has_legal_move = False
        
        for start, end in moves:
            # 模拟走棋
            captured = self.make_move(start, end)
            
            # 【重要】走完之后检查自己是否还在被将军（处理非法的逃生步）
            # 如果你的 get_all_moves 已经是伪合法的（可能包含送将），需要这一步
            # 如果你的 get_all_moves 严格保证合法，这步可省略，但在 QS 中通常是伪合法生成
            if self.is_in_check(maximizing_player):
                self.undo_move(start, end, captured)
                continue
            
            has_legal_move = True
            
            score = self.quiescence_search(alpha, beta, not maximizing_player, qs_depth + 1)
            
            self.undo_move(start, end, captured)
            
            if maximizing_player:
                if score >= beta: return beta
                if score > alpha: alpha = score
            else:
                if score <= alpha: return alpha
                if score < beta: beta = score

        # 6. 处理被将军但无棋可走的情况 (Checkmate)
        if in_check and not has_legal_move:
            # 这是一个绝杀局面
            # 返回一个极小值 (注意层数调整，越早被杀分越低)
            return -SCORE_INF + qs_depth if maximizing_player else SCORE_INF - qs_depth

        # 如果不是被将军，只是没有吃子步了，或者所有吃子步都亏，
        # alpha (max) 或 beta (min) 已经保留了 stand_pat 的值，直接返回即可
        return alpha if maximizing_player else beta

    # --- 新增：判断是否允许空步裁剪的辅助函数 ---
    def find_king(self, is_red_king):
        target = 'K' if is_red_king else 'k'
        for r in range(ROWS):
            for c in range(COLS):
                if self.board[r][c] == target:
                    return r, c
        return None

    def is_in_check(self, is_red_turn):
        """判断当前行动方是否被将军（简化版检测，用于NMP安全检查）"""
        # 找到己方老将
        king_pos = self.find_king(is_red_turn)
        if not king_pos: return True # 如果老将被吃，视为最差情况
        kr, kc = king_pos
        
        # 简单扫描：检查车、炮、马、兵是否攻击老将
        # 这是一个耗时操作，但在NMP中是必要的，否则会导致严重的漏杀
        
        # 1. 检查同行同列的车/炮/帅
        for dr, dc in [(0, 1), (0, -1), (1, 0), (-1, 0)]:
            nr, nc = kr + dr, kc + dc
            first_piece = None
            while self.in_board(nr, nc):
                p = self.board[nr][nc]
                if p != '.':
                    if first_piece is None:
                        first_piece = p
                        # 车或老将直接照面
                        if self.is_red(p) != is_red_turn:
                            if p.lower() in ['r', 'k']: return True
                    else:
                        # 翻山炮
                        if self.is_red(p) != is_red_turn:
                            if p.lower() == 'c': return True
                        break # 隔两个子以上无效
                nr, nc = nr + dr, nc + dc
        
        # 2. 检查马
        knight_checks = [(-2, -1), (-2, 1), (2, -1), (2, 1), (-1, -2), (-1, 2), (1, -2), (1, 2)]
        knight_legs   = [(-1, 0), (-1, 0), (1, 0), (1, 0), (0, -1), (0, 1), (0, -1), (0, 1)]
        for (dr, dc), (lr, lc) in zip(knight_checks, knight_legs):
            nr, nc = kr + dr, kc + dc
            lr, lc = kr + lr, kc + lc
            if self.in_board(nr, nc) and self.in_board(lr, lc):
                p = self.board[nr][nc]
                if p != '.' and self.is_red(p) != is_red_turn and p.lower() == 'n':
                    if self.board[lr][lc] == '.': # 必须无蹩脚
                        return True
                        
        # 3. 检查兵/卒 (老将只在九宫，只看周围一步即可)
        pawn_char = 'p' if is_red_turn else 'P' # 敌方兵的字符
        pawn_dir = 1 if is_red_turn else -1     # 敌兵进攻方向（相对于敌方是前进，相对于己方是后退）
        # 也就是检查 kr - pawn_dir 行是否有敌兵
        check_r = kr - pawn_dir
        if self.in_board(check_r, kc) and self.board[check_r][kc] == pawn_char: return True # 迎面冲来
        for dc in [-1, 1]: # 左右兵
            if self.in_board(kr, kc+dc) and self.board[kr][kc+dc] == pawn_char: return True

        return False

    def make_null_move(self):
        """执行空步：只交换出子权和Hash"""
        self.turn = 'black' if self.turn == 'red' else 'red'
        self.current_hash ^= self.zobrist_turn
        self.history.append(self.current_hash) # 新增

    def undo_null_move(self):
        """撤销空步：操作完全一样"""
        self.history.pop() # 新增
        self.turn = 'black' if self.turn == 'red' else 'red'
        self.current_hash ^= self.zobrist_turn
    def minimax(self, depth, alpha, beta, maximizing_player, allow_null=True):
        self.nodes += 1
        # --- 新增：检测重复局面 ---
        # 如果当前 Hash 在历史列表中出现的次数大于1（包含刚才 make_move 加入的那次），说明重复了
        # 这意味着：如果走了这一步，局面与之前的某个时候一模一样
        if self.history.count(self.current_hash) > 1:
            # 这是一个重复局面。
            # 通常判为和棋 (0分)。如果AI发现 0分 比输棋(-10000)好，它就会选择重复（长将保命）。
            # 如果AI有赢棋走法(+100)，它就不会走这一步。
            # 这样既避免了死循环，又符合象棋规则逻辑。
            return 0, None
        # 0. 检查超时
        if self.is_time_up():
            return 0, None

        # 1. 查表 (TT Lookup)
        original_alpha = alpha
        idx = self.current_hash % self.tt_size
        tt_entry = self.tt[idx]
        tt_move = None
        
        if tt_entry is not None and tt_entry[0] == self.current_hash:
            tt_hash, tt_depth, tt_flag, tt_score, tt_move = tt_entry
            # 只有当表里的深度比当前要求更深或相等时，结果才可靠
            if tt_depth >= depth:
                if tt_flag == TT_EXACT:
                    return tt_score, tt_move
                elif tt_flag == TT_ALPHA and tt_score <= alpha:
                    return tt_score, tt_move
                elif tt_flag == TT_BETA and tt_score >= beta:
                    return tt_score, tt_move

        # 2. 基础结束条件
        # 如果深度耗尽，进入静态搜索 (QS)
        if depth <= 0:
            val = self.quiescence_search(alpha, beta, maximizing_player)
            return val, None

        # 3. 检查胜负 (防止绝杀时死循环)
        kings = [False, False]
        # 这一步其实比较耗时，但在 Python 简易引擎中为了安全保留
        for r in range(ROWS):
            for c in range(COLS):
                if self.board[r][c] == 'K': kings[0] = True
                if self.board[r][c] == 'k': kings[1] = True
        if not kings[0]: return -SCORE_INF + depth, None 
        if not kings[1]: return SCORE_INF - depth, None 

        in_check = self.is_in_check(maximizing_player)
        
        # --- 移除死循环风险的 Check Extension ---
        # 原来的 if in_check: depth += 1 会导致无限递归。
        # 这里改为：如果被将军，我们不做空步裁剪(NMP)，
        # 并且依靠 QS 在 depth=0 时处理将军，或者仅在 depth 较小时才极少量延伸(这里为了稳定，暂不延伸)。

        # --- Null Move Pruning (空步裁剪) ---
        # 只有在：没被将军 + 深度足够 + 没到残局(简单判断) 时才启用
        if OPEN_NMP and depth >= 3 and not in_check and allow_null:
            self.make_null_move()
            
            # 动态 R 值计算 (保持不变)
            if depth > 6:
                R = 3
            else:
                R = 2
            
            # 确保剩下的深度至少为 0
            next_depth = max(0, depth - 1 - R)
            
            # --- 逻辑修正开始 ---
            if maximizing_player:
                # 当前是红方：希望证明 即使空步，局势依然 >= beta
                # 交给黑方搜，使用 (beta-1, beta) 窗口
                val, _ = self.minimax(next_depth, beta - 1, beta, False, allow_null=False)
                self.undo_null_move() # 记得恢复
                
                if self.stop_search: return 0, None
                if val >= beta and val < 20000: 
                    return beta, None
            else:
                # 当前是黑方：希望证明 即使空步，局势依然 <= alpha
                # 交给红方搜，使用 (alpha, alpha+1) 窗口
                val, _ = self.minimax(next_depth, alpha, alpha + 1, True, allow_null=False)
                self.undo_null_move() # 记得恢复

                if self.stop_search: return 0, None
                if val <= alpha and val > -20000: 
                    return alpha, None
            # --- 逻辑修正结束 ---

        # 4. 生成着法
        moves = self.get_all_moves(maximizing_player)
        if not moves:
            # 无棋可走：如果是被将军，就是输了；如果没被将军，是困毙(算和棋或输，这里简化为输)
            return (-SCORE_INF if maximizing_player else SCORE_INF), None

        # 排序
        killers = self.killer_moves[depth] if depth < 64 else [None, None]
        def move_sorter(m):
            start, end = m
            if tt_move and (start, end) == tt_move: return 2000000 # TT Move
            
            victim = self.board[end[0]][end[1]]
            if victim != '.': # MVV-LVA
                val = PIECE_VALUES.get(victim, 0)
                attacker = self.board[start[0]][start[1]]
                attacker_val = PIECE_VALUES.get(attacker, 0)
                return 100000 + val * 10 - attacker_val
            
            if m == killers[0]: return 90000
            if m == killers[1]: return 80000
            return self.history_table[start[0]][start[1]][end[0]][end[1]]

        moves.sort(key=move_sorter, reverse=True)#5 8在第12

        best_move = moves[0]
        best_score = -float(SCORE_INF) if maximizing_player else float(SCORE_INF)
        moves_count = 0
        
        # 5. 遍历
        for start, end in moves:
            moves_count += 1
            captured = self.make_move(start, end)
            
            score = 0
            is_killer = ((start, end) == killers[0] or (start, end) == killers[1])
            
            # --- PVS & LMR ---
            # 只有当不是被将军状态时，才敢大胆进行 LMR 裁剪
            do_lmr = (depth >= 3 and moves_count > 4 and 
                      captured == '.' and not in_check and 
                      not is_killer)
            
            if maximizing_player:
                if moves_count == 1:
                    score, _ = self.minimax(depth - 1, alpha, beta, False)
                else:
                    reduction = 1 if do_lmr else 0
                    if moves_count > 15 and do_lmr: reduction = 2
                    
                    search_depth = depth - 1 - reduction
                    # 确保深度不会变成负数导致逻辑混乱(虽然 minimax 入口有判断，但保持清醒很好)
                    if search_depth < 0: search_depth = 0

                    # 1. 尝试零窗口搜索
                    score, _ = self.minimax(search_depth, alpha, alpha + 1, False)
                    
                    # 2. 如果 Fail High (在这个深度居然比 alpha 好)，说明可能过度剪枝了
                    if score > alpha:
                        if do_lmr: # 恢复深度重搜
                            score, _ = self.minimax(depth - 1, alpha, alpha + 1, False)
                        if score > alpha and score < beta: # 全窗口重搜
                            score, _ = self.minimax(depth - 1, alpha, beta, False)
            else:
                if moves_count == 1:
                    score, _ = self.minimax(depth - 1, alpha, beta, True)
                else:
                    reduction = 1 if do_lmr else 0
                    if moves_count > 15 and do_lmr: reduction = 2
                    
                    search_depth = depth - 1 - reduction
                    if search_depth < 0: search_depth = 0

                    score, _ = self.minimax(search_depth, beta - 1, beta, True)
                    
                    if score < beta:
                        if do_lmr:
                            score, _ = self.minimax(depth - 1, beta - 1, beta, True)
                        if score < beta and score > alpha:
                            score, _ = self.minimax(depth - 1, alpha, beta, True)

            self.undo_move(start, end, captured)
            
            if self.stop_search: return 0, None

            # 更新 Alpha/Beta
            if maximizing_player:
                if score > best_score:
                    best_score = score
                    best_move = (start, end)
                    if best_score > alpha:
                        alpha = best_score
                        if alpha >= beta:
                            if captured == '.':
                                self.history_table[start[0]][start[1]][end[0]][end[1]] += depth * depth
                                if self.killer_moves[depth][0] != (start, end):
                                    self.killer_moves[depth][1] = self.killer_moves[depth][0]
                                    self.killer_moves[depth][0] = (start, end)
                            break
            else:
                if score < best_score:
                    best_score = score
                    best_move = (start, end)
                    if best_score < beta:
                        beta = best_score
                        if beta <= alpha:
                            if captured == '.':
                                self.history_table[start[0]][start[1]][end[0]][end[1]] += depth * depth
                                if self.killer_moves[depth][0] != (start, end):
                                    self.killer_moves[depth][1] = self.killer_moves[depth][0]
                                    self.killer_moves[depth][0] = (start, end)
                            break

        # 6. 存表
        flag = TT_EXACT
        if best_score <= original_alpha: flag = TT_ALPHA
        elif best_score >= beta: flag = TT_BETA
        
        if tt_entry is None or depth >= tt_entry[1]:
            self.tt[idx] = (self.current_hash, depth, flag, best_score, best_move)

        return best_score, best_move
    def search_main(self, max_time, is_ai_red):
        cloud_data = self.query_cloud_book()
        if cloud_data:
            book_move, book_score = cloud_data # 解构获取真实分数
            with open("log.txt", "a", encoding="utf-8") as f:
                print(f"使用云库走法: {book_move}, 云库分数: {book_score}", file=f)
            return book_score, book_move # 返回真实的分数和走法
        self.start_time = time.time()
        self.time_limit = max_time
        self.stop_search = False
        
        # 这两个变量存储【上一次完整深度】的结果
        last_completed_move = None
        last_completed_val = 0
        
        for depth in range(1, 64):
            # 尝试搜索当前深度
            current_val, current_move = self.minimax(depth, -float(SCORE_INF), float(SCORE_INF), is_ai_red)
            
            # 检查是否是因为超时导致的返回
            if self.stop_search or current_move is None:
                # 如果深度 6 搜了一半断了，我们依然有深度 5 的保底走法
                break 
            
            # 走到这里，说明当前深度【完全搜完了】，结果是可信的
            last_completed_move = current_move
            last_completed_val = current_val
            
            # 打印日志
            elapsed = time.time() - self.start_time
            with open("log.txt", "a", encoding="utf-8") as f:
                print(f"完成深度 {depth} | 耗时 {elapsed:.2f}s | 评估 {last_completed_val}", file=f)

            if abs(last_completed_val) > 20000: break # 发现绝杀
            if elapsed > max_time * 0.2: break # 剩余时间预警

        # 哪怕深度 6 失败了，我们返回的也是深度 5 的最佳走法
        return last_completed_val, last_completed_move
    def query_cloud_book(self):
        if not CLOUD_BOOK_ENABLED:
            return None
        """查询象棋云库并返回候选走法及分数"""
        fen = self.to_fen()
        encoded_fen = urllib.parse.quote(fen)
        url = f"http://www.chessdb.cn/chessdb.php?action=queryall&learn=1&board={encoded_fen}"
        try:
            with urllib.request.urlopen(url, timeout=2) as response:
                data = response.read().decode('utf-8')
                if "move:" not in data:
                    return None
                
                moves = []
                for line in data.split('|'):
                    parts = {item.split(':')[0]: item.split(':')[1] for item in line.split(',') if ':' in item}
                    if 'move' in parts and 'score' in parts:
                        moves.append({
                            'move': parts['move'],
                            'score': int(parts['score'])
                        })
                
                if not moves: return None
                
                # 筛选规则：分数不低于最高分 5 分
                max_score = moves[0]['score']
                candidates = [m for m in moves if m['score'] >= max_score - 5]
                
                # 随机选一个高分走法
                selected = random.choice(candidates)
                move_coords = self.uci_to_move(selected['move'])
                
                # 返回坐标和该走法对应的真实分数
                return move_coords, selected['score']
        except Exception as e:
            with open("log.txt", "a") as f:
                print(f"云库查询失败: {e}", file=f)
            return None
def start_engine(long_max_depth=LONG_MAX_DEPTH):
    engine = XiangqiCLI()

    print("ready", flush=True)
    cnt=0
    while True:
        try:
            cmd = input().strip()
        except EOFError:
            break

        if cmd == "quit":
            break

        if cmd.startswith("side"):
            # side red / side black
            _, s = cmd.split()
            engine.player_side = s

        elif cmd.startswith("move"):
            # move r1 c1 r2 c2
            _, r1, c1, r2, c2 = cmd.split()
            captured_piece = engine.make_move((int(r1),int(c1)), (int(r2),int(c2)))
            if captured_piece != '.':
                engine.history = [engine.current_hash]

        elif cmd.startswith("search"):
            t0 = time.time()
            cnt+=1
            is_ai_red = (engine.player_side == 'black')
            if USE_DEPTH:
                if cnt<=3:
                    DEPTH = 5
                else:
                    DEPTH = long_max_depth # 中后期加深到6层
                val, best = engine.minimax(DEPTH, -float(SCORE_INF ), float(SCORE_INF ), is_ai_red)
            else:
                MAX_TIME = 10.0 if  cnt<=3 else LONG_MAX_TIME # 每步最多思考 10 秒(仅在非固定深度时生效) 
                print(f">>> AI 正在思考 (限时 {MAX_TIME} 秒)...")
                val, best = engine.search_main(MAX_TIME, is_ai_red)

            if best:
                (r1,c1),(r2,c2) = best
                captured_piece=engine.make_move(best[0], best[1])
                if captured_piece != '.':
                    engine.history = [engine.current_hash]
                print(f"move {r1} {c1} {r2} {c2}", flush=True)
            else:
                print("resign", flush=True)
            with open("log.txt", "a", encoding="utf-8") as f:
                print(f"思考耗时: {time.time()-t0:.2f}s, 评估分: {val} ", file=f)



if __name__ == "__main__":
    if len(sys.argv) > 1:
        start_engine(int(sys.argv[1]))
    else:
        start_engine()

